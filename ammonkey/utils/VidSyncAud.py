'''
A new sync script based on audio track. Theoretically the most accurate and
robust way for us to sync. Considering as a cross validation w/ LED detection.

This script extracts short audio segments, computes their RMS energy envelopes,
and uses cross-correlation to determine the offsets that align each videoâ€™s 
audio in time.

Credit: ChatGPT
Revised by Mel
'''

import os
import numpy as np
import librosa
import subprocess
import scipy.signal
import matplotlib.pyplot as plt
from diskcache import Cache

ffmpeg_path = r'C:\ffmpeg\bin\ffmpeg.exe'
THRES_SNR = 3.2
THRES_PEAK = 1.16
THRES_PEAK_UNSURE = 1.6
DEBUG = False
cache = Cache('syncAud')

def extract_audio(video_path, sample_rate=48000, duration=30, start=0):
    """
    Extracts a short segment of audio from the video using FFmpeg.
    - duration: The number of seconds to extract (default: 30 s).
    - sample_rate: The target audio sampling rate (default: 48000 Hz).
    """
    if not os.path.exists(video_path):
        raise FileNotFoundError(f'In extract_audio: cannot find {video_path}')
    
    temp_audio = "mky_temp_audio.wav"
    cmd = [
        ffmpeg_path, "-y", "-i", video_path,
        "-ss", str(start),
        "-ac", "1", 
        "-ar", str(sample_rate),
        "-t", str(duration), 
        "-vn", "-loglevel", "error", temp_audio
    ]
    try:
        result = subprocess.run(cmd, stdout=subprocess.DEVNULL) #, stderr=subprocess.DEVNULL)
    except Exception as e:
        raise RuntimeError("Audio extraction failed. It's possibly because ffmpeg isn't correctly configured")
    if result.stderr:
        print(result.stderr)
    audio, sr = librosa.load(temp_audio, sr=sample_rate)
    os.remove(temp_audio)
    print(f'Extracted audio from {os.path.basename(video_path)}')
    return audio, sr

def compute_energy_envelope(audio, sr, hop_length=128):
    """
    Computes the energy envelope of the audio signal.
    - hop_length: The step size between frames for feature extraction.
    """
    energy = librosa.feature.rms(y=audio, hop_length=hop_length)[0]
    return energy

def find_best_sync_offset(ref_audio, target_audio, sr, fps=120, hop_length=128):
    """
    Finds the best synchronization offset between two audio signals using cross-correlation.
    - Converts the time offset into frame offset based on fps.
    """
    ref_energy = compute_energy_envelope(ref_audio, sr, hop_length)
    target_energy = compute_energy_envelope(target_audio, sr, hop_length)
    
    correlation = scipy.signal.correlate(target_energy, ref_energy, mode='full')
    #print(f'Paired corr {np.max(correlation)}, Mean corr {np.mean(correlation)}, max/mean = {np.max(correlation)/np.mean(correlation)}')
    

    lag = np.argmax(correlation) - (len(ref_energy) - 1)
    
    time_offset = lag * (hop_length / sr)  
    frame_offset = round(time_offset * fps)  
    # snr = np.max(correlation)/np.mean(correlation)
    dom, snr = has_dominant_peak(correlation, THRES_PEAK)
    if DEBUG:
        print(dom, snr)
        plt.figure(figsize=(10, 4))
        plt.plot(correlation)
        plt.show()
    if not dom:
        raise ValueError(f'Sync failed: possible false sync. Peak dominence: {snr}')
    
    return frame_offset, snr

# @cache.memoize(expire=14400)
def sync_videos(video_paths:list[str], fps=119.88, duration=30, start=0) -> dict[str, tuple[str, np.ndarray, int]]:
    """
    Synchronizes a set of videos based on their audio tracks.
    - Extracts only a short duration to speed up processing.
    - Returns the frame differences relative to the first video.
    """
    if len(video_paths) < 2:
        raise ValueError("At least two videos are required for synchronization.")
    
    print(f'Processing base video... Audio thres {THRES_PEAK}')
    ref_audio, sr = extract_audio(video_paths[0], duration=duration, start=start)
    sync_results = {"reference": (video_paths[0], ref_audio, 0)}

    for video in video_paths[1:]:
        print(f'Aligning video {os.path.basename(video)}...')
        target_audio, _ = extract_audio(video, duration=duration, start=start)
        try:
            frame_offset, snr = find_best_sync_offset(ref_audio, target_audio, sr, fps)
        except Exception as e:
            print(f'Failed syncing {os.path.basename(video)}: {e}')
            frame_offset = None
        sync_results[video] = (video, target_audio, frame_offset)

    return sync_results

def plot_synced_waveforms(sync_results, sr, fps=119.88, duration=5):
    """Plots the waveforms of all synced audio signals correctly aligned."""
    plt.figure(figsize=(10, len(sync_results) * 2))

    max_length = int(duration * sr)
    min_frame_offset = min([t[2] for t in sync_results.values()])  # Earliest starting point
    
    for i, (video, audio, frame_offset) in enumerate(sync_results.values()):
        start_sample = int(((frame_offset - min_frame_offset)/fps)*sr)
        
        if start_sample < 0:
            pad_length = abs(start_sample)
            aligned_audio = np.pad(audio[: max_length - pad_length], (pad_length, 0), mode='constant')
        else:
            aligned_audio = audio[start_sample : start_sample + max_length]
        
        time_axis = np.linspace(0, duration, len(aligned_audio))

        plt.subplot(len(sync_results), 1, i + 1)
        plt.plot(time_axis, aligned_audio, label=f"{video} (Offset: {(frame_offset/fps):.3f}s)")
        plt.legend()
        plt.xlabel("Time (seconds)")
        plt.ylabel("Amplitude")
    
    plt.suptitle("Synced Audio Waveforms (Aligned Correctly)")
    plt.tight_layout()
    plt.show()

def save_synced_waveforms(sync_results, sr, fps=119.88, duration=5, tgt_path=''):
    """Plots and saves the waveforms of all synced audio waves."""
    plt.figure(figsize=(10, len(sync_results) * 2))

    max_length = int(duration * sr)
    min_frame_offset = min([t[2] for t in sync_results.values()])  # Earliest starting point
    
    for i, (video, audio, frame_offset) in enumerate(sync_results.values()):
        if frame_offset is None:
            continue

        start_sample = int(((frame_offset - min_frame_offset)/fps)*sr)
        
        if start_sample < 0:
            pad_length = abs(start_sample)
            aligned_audio = np.pad(audio[: max_length - pad_length], (pad_length, 0), mode='constant')
        else:
            aligned_audio = audio[start_sample : start_sample + max_length]
        
        time_axis = np.linspace(0, duration, len(aligned_audio))

        plt.subplot(len(sync_results), 1, i + 1)
        plt.plot(time_axis, aligned_audio, label=f"{video} (Offset: {(frame_offset/fps):.3f}s)")
        plt.legend()
        plt.xlabel("Time (seconds)")
        plt.ylabel("Amplitude")
    
    plt.suptitle("Synced Audio Waveforms (Aligned Correctly)")
    plt.tight_layout()
    try:
        plt.savefig(os.path.join(tgt_path,f'audio_comp_{os.path.basename(sync_results["reference"][0]).split(".")[0]}.jpg'))
    except Exception as e:
        print(f'[ERROR] Sync result plot is not saved for {sync_results["reference"][-1]}: {e}')
    finally:
        plt.close()

def has_dominant_peak(correlation: np.array, ratio_thresh: float = THRES_PEAK) -> tuple[bool, float]:
    from scipy.signal import find_peaks

    peaks, _ = find_peaks(correlation)
    if len(peaks) < 2:
        return False

    heights = correlation[peaks]
    top_two = np.partition(heights, -2)[-2:]
    return top_two[1] / top_two[0] > ratio_thresh, top_two[1] / top_two[0]

if __name__ == '__main__':
    video_files = [
        r'D:\AmbiguousMonkey\errVids\C0523.mp4',
        r'D:\AmbiguousMonkey\errVids\C0471.mp4',
        r'D:\AmbiguousMonkey\errVids\C0684.mp4',
        r'D:\AmbiguousMonkey\errVids\C0643.mp4',
    ]
    '''
    video_files = [
        r'D:\AmbiguousMonkey\testsync\C0565.mp4',
        r'D:\AmbiguousMonkey\testsync\C0737.mp4',
        r'D:\AmbiguousMonkey\testsync\C0778.mp4',
        r'D:\AmbiguousMonkey\testsync\C0617.mp4',
    ]
    video_files = [
        r'D:\AmbiguousMonkey\testsync\C0567.mp4',
        r'D:\AmbiguousMonkey\testsync\C0739.mp4',
        r'D:\AmbiguousMonkey\testsync\C0780.mp4',
        r'D:\AmbiguousMonkey\testsync\C0619.mp4',
        r'D:\AmbiguousMonkey\testsync\C0565.mp4',
        r'D:\AmbiguousMonkey\testsync\C0737.mp4',
        r'D:\AmbiguousMonkey\testsync\C0778.mp4',
        r'D:\AmbiguousMonkey\testsync\C0617.mp4',
    ]'''
    video_files = [
        "P:\\projects\\monkeys\\Chronic_VLL\\DATA_RAW\\Pici\\2025\\03\\20250326\\cam1\\C0644.mp4",
        "P:\\projects\\monkeys\\Chronic_VLL\\DATA_RAW\\Pici\\2025\\03\\20250326\\cam2\\C0592.mp4",
        "P:\\projects\\monkeys\\Chronic_VLL\\DATA_RAW\\Pici\\2025\\03\\20250326\\cam3\\C0805.mp4",
        "P:\\projects\\monkeys\\Chronic_VLL\\DATA_RAW\\Pici\\2025\\03\\20250326\\cam4\\C0764.mp4",
    ]
    frame_shifts = sync_videos(video_files, fps=119.88, duration=80)
    print([i[-1] for i in frame_shifts.values()])
    save_synced_waveforms(frame_shifts, 48000, 119.88,
                           duration=30, 
                           tgt_path='D:\AmbiguousMonkey\errVids',
                           )